framework: torch
env: path_planning
lambda: 0.95
kl_coeff: 0.5
kl_target: 0.01
clip_rewards: True
clip_param: 0.2
vf_clip_param: 250.0
vf_share_layers: False
vf_loss_coeff: 1.0e-4
entropy_coeff: 0.01
train_batch_size: 5000
rollout_fragment_length: 100
sgd_minibatch_size: 1000
num_sgd_iter: 5
num_workers: 16
num_envs_per_worker: 8
lr: 4.0e-4
gamma: 0.99
batch_mode: complete_episodes
observation_filter: NoFilter
num_gpus: 1
model:
    custom_model: adversarial
    custom_model_config:
        graph_layers: 1
        graph_tabs: 3
        graph_edge_features: 1

        # state 24
        graph_features: 128
        cnn_filters: [[8, [4, 4], 2], [16, [4, 4], 2], [32, [4, 4], 2]]
        value_cnn_filters: [[32, [4, 4], 2], [64, [4, 4], 2]]
        value_cnn_compression: 256
        cnn_compression: 128
        
        # 7x7
        #graph_features: 32
        #cnn_filters: [[16, [4, 4], 2], [32, [4, 4], 2]]
        #value_cnn_filters: [[16, [4, 4], 2], [32, [4, 4], 2]]
        #value_cnn_compression: 128
        #cnn_compression: 32
        
        # state 48
        #graph_features: 512
        #cnn_filters: [[32, [8, 8], 4], [64, [4, 4], 2], [128, [4, 4], 2]]
        #value_cnn_filters: [[32, [8, 8], 2], [64, [4, 4], 2], [128, [4, 4], 2]]
        #value_cnn_compression: 512
        #cnn_compression: 512
        
        relative: true
        #lstm_cell_size: 64
        #lstm_layers: 1
        activation: relu
        #position: none
        freeze_coop: False
        freeze_greedy: False
        freeze_coop_value: False
        freeze_greedy_value: False
        cnn_residual: False
        agent_split: 1
env_config:
    world_shape: [12, 12]
    state_size: 24
    max_episode_len: 50
    n_agents: [1, 15]
    disabled_teams_step: [True, False]
    disabled_teams_comms: [True, False]
    communication_range: 5.0
    ensure_connectivity: True
    reward_type: coop_only
    world_mode: warehouse
    agents:
        visibility_distance: 0
        relative_coord_frame: True
evaluation_num_workers: 1
evaluation_interval: 1
evaluation_num_episodes: 10
evaluation_config:
    env_config:
        reward_type: local
_use_trajectory_view_api: False
alternative_config:
    self_interested:
        env_config:
            reward_type: greedy_only
            disabled_teams_step: [False, False]
            disabled_teams_comms: [False, False]
        model:
            custom_options:
                freeze_coop: True
                freeze_greedy: False
        evaluation_config:
            env_config:
                reward_type: local
    re_adapt:
        env_config:
            reward_type: coop_only
            disabled_teams_step: [False, False]
            disabled_teams_comms: [False, False]
        model:
            custom_options:
                freeze_coop: False
                freeze_greedy: True
        evaluation_config:
            env_config:
                reward_type: local
